{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30458,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic competition with TensorFlow Decision Forests\n\nThis notebook will take you through the steps needed to train a baseline Gradient Boosted Trees Model using TensorFlow Decision Forests and creating a submission on the Titanic competition. \n\nThis notebook shows:\n\n1. How to do some basic pre-processing. For example, the passenger names will be tokenized, and ticket names will be splitted in parts.\n1. How to train a Gradient Boosted Trees (GBT) with default parameters\n1. How to train a GBT with improved default parameters\n1. How to tune the parameters of a GBTs\n1. How to train and ensemble many GBTs","metadata":{}},{"cell_type":"markdown","source":"# Imports dependencies","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\n\nprint(f\"Found TF-DF {tfdf.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-11T11:06:17.042709Z","iopub.execute_input":"2024-02-11T11:06:17.045078Z","iopub.status.idle":"2024-02-11T11:06:30.158478Z","shell.execute_reply.started":"2024-02-11T11:06:17.044996Z","shell.execute_reply":"2024-02-11T11:06:30.156415Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Found TF-DF 1.2.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load dataset","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nserving_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\ntrain_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T11:07:02.356614Z","iopub.execute_input":"2024-02-11T11:07:02.357667Z","iopub.status.idle":"2024-02-11T11:07:02.435716Z","shell.execute_reply.started":"2024-02-11T11:07:02.357595Z","shell.execute_reply":"2024-02-11T11:07:02.434327Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n5            6         0       3   \n6            7         0       1   \n7            8         0       3   \n8            9         1       3   \n9           10         1       2   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n5                                   Moran, Mr. James    male   NaN      0   \n6                            McCarthy, Mr. Timothy J    male  54.0      0   \n7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n5      0            330877   8.4583   NaN        Q  \n6      0             17463  51.8625   E46        S  \n7      1            349909  21.0750   NaN        S  \n8      2            347742  11.1333   NaN        S  \n9      0            237736  30.0708   NaN        C  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Moran, Mr. James</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330877</td>\n      <td>8.4583</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>McCarthy, Mr. Timothy J</td>\n      <td>male</td>\n      <td>54.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>17463</td>\n      <td>51.8625</td>\n      <td>E46</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Palsson, Master. Gosta Leonard</td>\n      <td>male</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>349909</td>\n      <td>21.0750</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n      <td>female</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>347742</td>\n      <td>11.1333</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>1</td>\n      <td>2</td>\n      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n      <td>female</td>\n      <td>14.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>237736</td>\n      <td>30.0708</td>\n      <td>NaN</td>\n      <td>C</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Prepare dataset\n\nWe will apply the following transformations on the dataset.\n\n1. Tokenize the names. For example, \"Braund, Mr. Owen Harris\" will become [\"Braund\", \"Mr.\", \"Owen\", \"Harris\"].\n2. Extract any prefix in the ticket. For example ticket \"STON/O2. 3101282\" will become \"STON/O2.\" and 3101282.","metadata":{}},{"cell_type":"code","source":"def preprocess(df):\n    df = df.copy()\n    \n    def normalize_name(x):\n        return \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n    \n    # takes the last item of the ticket array; should always give an integer\n    def ticket_number(x):\n        return x.split(\" \")[-1]\n    \n    # creates a new string from the remaining ticket array\n    # concatenates if necessary\n    # make sure to not return empty strings, but rather NONE\n    def ticket_item(x):\n        items = x.split(\" \")\n        if len(items) == 1:\n            return \"NONE\"\n        return \"_\".join(items[0:-1])\n    \n    df[\"Name\"] = df[\"Name\"].apply(normalize_name)\n    df[\"Ticket_number\"] = df[\"Ticket\"].apply(ticket_number)\n    df[\"Ticket_item\"] = df[\"Ticket\"].apply(ticket_item)                     \n    return df\n    \npreprocessed_train_df = preprocess(train_df)\npreprocessed_serving_df = preprocess(serving_df)\n\npreprocessed_train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T11:07:06.992793Z","iopub.execute_input":"2024-02-11T11:07:06.994093Z","iopub.status.idle":"2024-02-11T11:07:07.055635Z","shell.execute_reply.started":"2024-02-11T11:07:06.994024Z","shell.execute_reply":"2024-02-11T11:07:07.053507Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                              Name     Sex   Age  SibSp  \\\n0                            Braund Mr Owen Harris    male  22.0      1   \n1  Cumings Mrs John Bradley Florence Briggs Thayer  female  38.0      1   \n2                             Heikkinen Miss Laina  female  26.0      0   \n3         Futrelle Mrs Jacques Heath Lily May Peel  female  35.0      1   \n4                           Allen Mr William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked Ticket_number Ticket_item  \n0      0         A/5 21171   7.2500   NaN        S         21171         A/5  \n1      0          PC 17599  71.2833   C85        C         17599          PC  \n2      0  STON/O2. 3101282   7.9250   NaN        S       3101282    STON/O2.  \n3      0            113803  53.1000  C123        S        113803        NONE  \n4      0            373450   8.0500   NaN        S        373450        NONE  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>Ticket_number</th>\n      <th>Ticket_item</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund Mr Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>21171</td>\n      <td>A/5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings Mrs John Bradley Florence Briggs Thayer</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n      <td>17599</td>\n      <td>PC</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen Miss Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>3101282</td>\n      <td>STON/O2.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle Mrs Jacques Heath Lily May Peel</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n      <td>113803</td>\n      <td>NONE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen Mr William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>373450</td>\n      <td>NONE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's keep the list of the input features of the model. Notably, we don't want to train our model on the \"PassengerId\" and \"Ticket\" features.","metadata":{}},{"cell_type":"code","source":"input_features = list(preprocessed_train_df.columns)\ninput_features.remove(\"Ticket\")\ninput_features.remove(\"PassengerId\")\ninput_features.remove(\"Survived\")\n#input_features.remove(\"Ticket_number\")\n\nprint(f\"Input features: {input_features}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-11T11:07:10.912591Z","iopub.execute_input":"2024-02-11T11:07:10.913211Z","iopub.status.idle":"2024-02-11T11:07:10.925380Z","shell.execute_reply.started":"2024-02-11T11:07:10.913152Z","shell.execute_reply":"2024-02-11T11:07:10.923327Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Input features: ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked', 'Ticket_number', 'Ticket_item']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Convert Pandas dataset to TensorFlow Dataset","metadata":{}},{"cell_type":"code","source":"def tokenize_names(features, labels=None):\n    \"\"\"Divide the names into tokens. TF-DF can consume text tokens natively.\"\"\"\n    features[\"Name\"] =  tf.strings.split(features[\"Name\"])\n    return features, labels\n\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_train_df,label=\"Survived\").map(tokenize_names)\nserving_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_serving_df).map(tokenize_names)","metadata":{"execution":{"iopub.status.busy":"2024-02-11T11:07:13.831412Z","iopub.execute_input":"2024-02-11T11:07:13.832232Z","iopub.status.idle":"2024-02-11T11:07:14.229915Z","shell.execute_reply.started":"2024-02-11T11:07:13.832185Z","shell.execute_reply":"2024-02-11T11:07:14.228178Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Train model with default parameters\n\n### Train model\n\nFirst, we are training a GradientBoostedTreesModel model with the default parameters.","metadata":{}},{"cell_type":"code","source":"model = tfdf.keras.GradientBoostedTreesModel(\n    # let's increase verbosity to get some insights\n    verbose=5, # increased to 5\n    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n    exclude_non_specified_features=True, # Only use the features in \"features\"\n    random_seed=1234,\n)\nmodel.fit(train_ds)\n\nself_evaluation = model.make_inspector().evaluation()\nprint(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-11T11:08:28.039406Z","iopub.execute_input":"2024-02-11T11:08:28.039996Z","iopub.status.idle":"2024-02-11T11:08:41.056130Z","shell.execute_reply.started":"2024-02-11T11:08:28.039916Z","shell.execute_reply":"2024-02-11T11:08:41.054918Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Use 4 thread(s) for training\nUse /tmp/tmpayj1me9l as temporary training directory\nReading training dataset...\nTraining tensor examples:\nFeatures: {'PassengerId': <tf.Tensor 'data_7:0' shape=(None,) dtype=int64>, 'Pclass': <tf.Tensor 'data_8:0' shape=(None,) dtype=int64>, 'Name': tf.RaggedTensor(values=Tensor(\"data_4:0\", shape=(None,), dtype=string), row_splits=Tensor(\"data_5:0\", shape=(None,), dtype=int64)), 'Sex': <tf.Tensor 'data_9:0' shape=(None,) dtype=string>, 'Age': <tf.Tensor 'data:0' shape=(None,) dtype=float64>, 'SibSp': <tf.Tensor 'data_10:0' shape=(None,) dtype=int64>, 'Parch': <tf.Tensor 'data_6:0' shape=(None,) dtype=int64>, 'Ticket': <tf.Tensor 'data_11:0' shape=(None,) dtype=string>, 'Fare': <tf.Tensor 'data_3:0' shape=(None,) dtype=float64>, 'Cabin': <tf.Tensor 'data_1:0' shape=(None,) dtype=string>, 'Embarked': <tf.Tensor 'data_2:0' shape=(None,) dtype=string>, 'Ticket_number': <tf.Tensor 'data_13:0' shape=(None,) dtype=string>, 'Ticket_item': <tf.Tensor 'data_12:0' shape=(None,) dtype=string>}\nLabel: Tensor(\"data_14:0\", shape=(None,), dtype=int64)\nWeights: None\nNormalized tensor features:\n {'Pclass': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast:0' shape=(None,) dtype=float32>), 'Name': SemanticTensor(semantic=<Semantic.CATEGORICAL_SET: 4>, tensor=tf.RaggedTensor(values=Tensor(\"data_4:0\", shape=(None,), dtype=string), row_splits=Tensor(\"data_5:0\", shape=(None,), dtype=int64))), 'Sex': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_9:0' shape=(None,) dtype=string>), 'Age': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_1:0' shape=(None,) dtype=float32>), 'SibSp': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_2:0' shape=(None,) dtype=float32>), 'Parch': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_3:0' shape=(None,) dtype=float32>), 'Fare': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_4:0' shape=(None,) dtype=float32>), 'Cabin': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_1:0' shape=(None,) dtype=string>), 'Embarked': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_2:0' shape=(None,) dtype=string>), 'Ticket_number': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_13:0' shape=(None,) dtype=string>), 'Ticket_item': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_12:0' shape=(None,) dtype=string>)}\nTraining dataset read in 0:00:08.398802. Found 891 examples.\nTraining model...\nStandard output detected as not visible to the user e.g. running in a notebook. Creating a training log redirection. If training gets stuck, try calling tfdf.keras.set_training_logs_redirection(False).\n","output_type":"stream"},{"name":"stderr","text":"[INFO 2024-02-11T11:08:36.544690742+00:00 kernel.cc:756] Start Yggdrasil model training\n[INFO 2024-02-11T11:08:36.545581668+00:00 kernel.cc:757] Collect training examples\n[INFO 2024-02-11T11:08:36.549506625+00:00 kernel.cc:388] Number of batches: 1\n[INFO 2024-02-11T11:08:36.549540843+00:00 kernel.cc:389] Number of examples: 891\n[INFO 2024-02-11T11:08:36.551212893+00:00 data_spec_inference.cc:303] 147 item(s) have been pruned (i.e. they are considered out of dictionary) for the column Cabin (0 item(s) left) because min_value_count=5 and max_number_of_unique_values=2000\n[INFO 2024-02-11T11:08:36.552157321+00:00 data_spec_inference.cc:303] 1441 item(s) have been pruned (i.e. they are considered out of dictionary) for the column Name (85 item(s) left) because min_value_count=5 and max_number_of_unique_values=2000\n[INFO 2024-02-11T11:08:36.552603357+00:00 data_spec_inference.cc:303] 28 item(s) have been pruned (i.e. they are considered out of dictionary) for the column Ticket_item (16 item(s) left) because min_value_count=5 and max_number_of_unique_values=2000\n[INFO 2024-02-11T11:08:36.553037088+00:00 data_spec_inference.cc:303] 671 item(s) have been pruned (i.e. they are considered out of dictionary) for the column Ticket_number (8 item(s) left) because min_value_count=5 and max_number_of_unique_values=2000\n[INFO 2024-02-11T11:08:36.553969123+00:00 kernel.cc:774] Training dataset:\nNumber of records: 891\nNumber of columns: 12\n\nNumber of columns by type:\n\tCATEGORICAL: 6 (50%)\n\tNUMERICAL: 5 (41.6667%)\n\tCATEGORICAL_SET: 1 (8.33333%)\n\nColumns:\n\nCATEGORICAL: 6 (50%)\n\t1: \"Cabin\" CATEGORICAL num-nas:687 (77.1044%) has-dict vocab-size:1 num-oods:147 (72.0588%)\n\t2: \"Embarked\" CATEGORICAL num-nas:2 (0.224467%) has-dict vocab-size:4 zero-ood-items most-frequent:\"S\" 644 (72.4409%)\n\t7: \"Sex\" CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:\"male\" 577 (64.7587%)\n\t9: \"Ticket_item\" CATEGORICAL has-dict vocab-size:17 num-oods:28 (3.14254%) most-frequent:\"NONE\" 665 (74.6352%)\n\t10: \"Ticket_number\" CATEGORICAL has-dict vocab-size:9 num-oods:671 (75.3086%) most-frequent:\"<OOD>\" 671 (75.3086%)\n\t11: \"__LABEL\" CATEGORICAL integerized vocab-size:3 no-ood-item\n\nNUMERICAL: 5 (41.6667%)\n\t0: \"Age\" NUMERICAL num-nas:177 (19.8653%) mean:29.6991 min:0.42 max:80 sd:14.5163\n\t3: \"Fare\" NUMERICAL mean:32.2042 min:0 max:512.329 sd:49.6655\n\t5: \"Parch\" NUMERICAL mean:0.381594 min:0 max:6 sd:0.805605\n\t6: \"Pclass\" NUMERICAL mean:2.30864 min:1 max:3 sd:0.835602\n\t8: \"SibSp\" NUMERICAL mean:0.523008 min:0 max:8 sd:1.10212\n\nCATEGORICAL_SET: 1 (8.33333%)\n\t4: \"Name\" CATEGORICAL_SET has-dict vocab-size:86 num-oods:1441 (161.728%) most-frequent:\"<OOD>\" 1441 (161.728%)\n\nTerminology:\n\tnas: Number of non-available (i.e. missing) values.\n\tood: Out of dictionary.\n\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n\ttokenized: The attribute value is obtained through tokenization.\n\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n\tvocab-size: Number of unique values.\n\n[INFO 2024-02-11T11:08:36.556816019+00:00 kernel.cc:790] Configure learner\n[INFO 2024-02-11T11:08:36.561147536+00:00 kernel.cc:804] Training config:\nlearner: \"GRADIENT_BOOSTED_TREES\"\nfeatures: \"^Age$\"\nfeatures: \"^Cabin$\"\nfeatures: \"^Embarked$\"\nfeatures: \"^Fare$\"\nfeatures: \"^Name$\"\nfeatures: \"^Parch$\"\nfeatures: \"^Pclass$\"\nfeatures: \"^Sex$\"\nfeatures: \"^SibSp$\"\nfeatures: \"^Ticket_item$\"\nfeatures: \"^Ticket_number$\"\nlabel: \"^__LABEL$\"\ntask: CLASSIFICATION\nrandom_seed: 1234\nmetadata {\n  framework: \"TF Keras\"\n}\npure_serving_model: false\n[yggdrasil_decision_forests.model.gradient_boosted_trees.proto.gradient_boosted_trees_config] {\n  num_trees: 300\n  decision_tree {\n    max_depth: 6\n    min_examples: 5\n    in_split_min_examples_check: true\n    keep_non_leaf_label_distribution: true\n    num_candidate_attributes: -1\n    missing_value_policy: GLOBAL_IMPUTATION\n    allow_na_conditions: false\n    categorical_set_greedy_forward {\n      sampling: 0.1\n      max_num_items: -1\n      min_item_frequency: 1\n    }\n    growing_strategy_local {\n    }\n    categorical {\n      cart {\n      }\n    }\n    axis_aligned_split {\n    }\n    internal {\n      sorting_strategy: PRESORTED\n    }\n    uplift {\n      min_examples_in_treatment: 5\n      split_score: KULLBACK_LEIBLER\n    }\n  }\n  shrinkage: 0.1\n  loss: DEFAULT\n  validation_set_ratio: 0.1\n  validation_interval_in_trees: 1\n  early_stopping: VALIDATION_LOSS_INCREASE\n  early_stopping_num_trees_look_ahead: 30\n  l2_regularization: 0\n  lambda_loss: 1\n  mart {\n  }\n  adapt_subsample_for_maximum_training_duration: false\n  l1_regularization: 0\n  use_hessian_gain: false\n  l2_regularization_categorical: 1\n  stochastic_gradient_boosting {\n    ratio: 1\n  }\n  apply_link_function: true\n  compute_permutation_variable_importance: false\n  binary_focal_loss_options {\n    misprediction_exponent: 2\n    positive_sample_coefficient: 0.5\n  }\n  early_stopping_initial_iteration: 10\n}\n\n[INFO 2024-02-11T11:08:36.563409101+00:00 kernel.cc:807] Deployment config:\ncache_path: \"/tmp/tmpayj1me9l/working_cache\"\nnum_threads: 4\ntry_resume_training: true\n\n[INFO 2024-02-11T11:08:36.563666719+00:00 kernel.cc:868] Train model\n[INFO 2024-02-11T11:08:36.999508118+00:00 early_stopping.cc:53] Early stop of the training because the validation loss does not decrease anymore. Best valid-loss: 0.860894\n[INFO 2024-02-11T11:08:37.007627113+00:00 kernel.cc:905] Export model in log directory: /tmp/tmpayj1me9l with prefix 0352ac675ae64929\n[INFO 2024-02-11T11:08:37.010427975+00:00 kernel.cc:923] Save model in resources\n[INFO 2024-02-11T11:08:37.020167037+00:00 abstract_model.cc:849] Model self evaluation:\nTask: CLASSIFICATION\nLabel: __LABEL\nLoss (BINOMIAL_LOG_LIKELIHOOD): 0.860894\n\nAccuracy: 0.826087  CI95[W][0 1]\nErrorRate: : 0.173913\n\n\nConfusion Table:\ntruth\\prediction\n   0   1   2\n0  0   0   0\n1  0  47   3\n2  0  13  29\nTotal: 92\n\nOne vs other classes:\n\n[INFO 2024-02-11T11:08:37.066761397+00:00 kernel.cc:1214] Loading model from path /tmp/tmpayj1me9l/model/ with prefix 0352ac675ae64929\n[INFO 2024-02-11T11:08:37.079070778+00:00 abstract_model.cc:1311] Engine \"GradientBoostedTreesQuickScorerExtended\" built\n[INFO 2024-02-11T11:08:37.079269755+00:00 kernel.cc:1046] Use fast generic engine\n","output_type":"stream"},{"name":"stdout","text":"Model trained in 0:00:00.554607\nCompiling model...\nWARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7964795475f0> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: could not get source code\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nModel compiled.\nAccuracy: 0.8260869383811951 Loss:0.8608942627906799\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train model with improved default parameters\n\nNow you'll use some specific parameters when creating the GBT model","metadata":{}},{"cell_type":"code","source":"model = tfdf.keras.GradientBoostedTreesModel(\n    verbose=5, # Very few logs\n    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n    exclude_non_specified_features=True, # Only use the features in \"features\"\n    \n    #num_trees=2000,\n    \n    # Only for GBT.\n    # A bit slower, but great to understand the model.\n    compute_permutation_variable_importance=True,\n    \n    # Change the default hyper-parameters\n    hyperparameter_template=\"benchmark_rank1@v1\",\n    \n    #num_trees=1000,\n    #tuner=tuner,\n    \n    min_examples=1,\n    categorical_algorithm=\"RANDOM\",\n    #max_depth=4,\n    shrinkage=0.05,\n    #num_candidate_attributes_ratio=0.2,\n    split_axis=\"SPARSE_OBLIQUE\",\n    sparse_oblique_normalization=\"MIN_MAX\",\n    sparse_oblique_num_projections_exponent=2.0,\n    num_trees=2000,\n    #validation_ratio=0.0,\n    random_seed=1234,\n    \n)\nmodel.fit(train_ds)\n\nself_evaluation = model.make_inspector().evaluation()\nprint(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-11T11:10:07.516124Z","iopub.execute_input":"2024-02-11T11:10:07.516642Z","iopub.status.idle":"2024-02-11T11:10:09.570077Z","shell.execute_reply.started":"2024-02-11T11:10:07.516599Z","shell.execute_reply":"2024-02-11T11:10:09.568571Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Resolve hyper-parameter template \"benchmark_rank1@v1\" to \"benchmark_rank1@v1\" -> {'growing_strategy': 'BEST_FIRST_GLOBAL', 'categorical_algorithm': 'RANDOM', 'split_axis': 'SPARSE_OBLIQUE', 'sparse_oblique_normalization': 'MIN_MAX', 'sparse_oblique_num_projections_exponent': 1.0}.\nUse 4 thread(s) for training\nUse /tmp/tmp43nud6l9 as temporary training directory\nReading training dataset...\nTraining tensor examples:\nFeatures: {'PassengerId': <tf.Tensor 'data_7:0' shape=(None,) dtype=int64>, 'Pclass': <tf.Tensor 'data_8:0' shape=(None,) dtype=int64>, 'Name': tf.RaggedTensor(values=Tensor(\"data_4:0\", shape=(None,), dtype=string), row_splits=Tensor(\"data_5:0\", shape=(None,), dtype=int64)), 'Sex': <tf.Tensor 'data_9:0' shape=(None,) dtype=string>, 'Age': <tf.Tensor 'data:0' shape=(None,) dtype=float64>, 'SibSp': <tf.Tensor 'data_10:0' shape=(None,) dtype=int64>, 'Parch': <tf.Tensor 'data_6:0' shape=(None,) dtype=int64>, 'Ticket': <tf.Tensor 'data_11:0' shape=(None,) dtype=string>, 'Fare': <tf.Tensor 'data_3:0' shape=(None,) dtype=float64>, 'Cabin': <tf.Tensor 'data_1:0' shape=(None,) dtype=string>, 'Embarked': <tf.Tensor 'data_2:0' shape=(None,) dtype=string>, 'Ticket_number': <tf.Tensor 'data_13:0' shape=(None,) dtype=string>, 'Ticket_item': <tf.Tensor 'data_12:0' shape=(None,) dtype=string>}\nLabel: Tensor(\"data_14:0\", shape=(None,), dtype=int64)\nWeights: None\nNormalized tensor features:\n {'Pclass': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast:0' shape=(None,) dtype=float32>), 'Name': SemanticTensor(semantic=<Semantic.CATEGORICAL_SET: 4>, tensor=tf.RaggedTensor(values=Tensor(\"data_4:0\", shape=(None,), dtype=string), row_splits=Tensor(\"data_5:0\", shape=(None,), dtype=int64))), 'Sex': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_9:0' shape=(None,) dtype=string>), 'Age': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_1:0' shape=(None,) dtype=float32>), 'SibSp': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_2:0' shape=(None,) dtype=float32>), 'Parch': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_3:0' shape=(None,) dtype=float32>), 'Fare': SemanticTensor(semantic=<Semantic.NUMERICAL: 1>, tensor=<tf.Tensor 'Cast_4:0' shape=(None,) dtype=float32>), 'Cabin': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_1:0' shape=(None,) dtype=string>), 'Embarked': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_2:0' shape=(None,) dtype=string>), 'Ticket_number': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_13:0' shape=(None,) dtype=string>), 'Ticket_item': SemanticTensor(semantic=<Semantic.CATEGORICAL: 2>, tensor=<tf.Tensor 'data_12:0' shape=(None,) dtype=string>)}\nTraining dataset read in 0:00:00.366709. Found 891 examples.\nTraining model...\n","output_type":"stream"},{"name":"stderr","text":"[INFO 2024-02-11T11:10:07.931443618+00:00 kernel.cc:756] Start Yggdrasil model training\n[INFO 2024-02-11T11:10:07.931488828+00:00 kernel.cc:757] Collect training examples\n[INFO 2024-02-11T11:10:07.931896556+00:00 kernel.cc:388] Number of batches: 1\n[INFO 2024-02-11T11:10:07.931915182+00:00 kernel.cc:389] Number of examples: 891\n[INFO 2024-02-11T11:10:07.933531885+00:00 data_spec_inference.cc:303] 147 item(s) have been pruned (i.e. they are considered out of dictionary) for the column Cabin (0 item(s) left) because min_value_count=5 and max_number_of_unique_values=2000\n[INFO 2024-02-11T11:10:07.934585667+00:00 data_spec_inference.cc:303] 1441 item(s) have been pruned (i.e. they are considered out of dictionary) for the column Name (85 item(s) left) because min_value_count=5 and max_number_of_unique_values=2000\n[INFO 2024-02-11T11:10:07.935159412+00:00 data_spec_inference.cc:303] 28 item(s) have been pruned (i.e. they are considered out of dictionary) for the column Ticket_item (16 item(s) left) because min_value_count=5 and max_number_of_unique_values=2000\n[INFO 2024-02-11T11:10:07.93568228+00:00 data_spec_inference.cc:303] 671 item(s) have been pruned (i.e. they are considered out of dictionary) for the column Ticket_number (8 item(s) left) because min_value_count=5 and max_number_of_unique_values=2000\n[INFO 2024-02-11T11:10:07.937021826+00:00 kernel.cc:774] Training dataset:\nNumber of records: 891\nNumber of columns: 12\n\nNumber of columns by type:\n\tCATEGORICAL: 6 (50%)\n\tNUMERICAL: 5 (41.6667%)\n\tCATEGORICAL_SET: 1 (8.33333%)\n\nColumns:\n\nCATEGORICAL: 6 (50%)\n\t1: \"Cabin\" CATEGORICAL num-nas:687 (77.1044%) has-dict vocab-size:1 num-oods:147 (72.0588%)\n\t2: \"Embarked\" CATEGORICAL num-nas:2 (0.224467%) has-dict vocab-size:4 zero-ood-items most-frequent:\"S\" 644 (72.4409%)\n\t7: \"Sex\" CATEGORICAL has-dict vocab-size:3 zero-ood-items most-frequent:\"male\" 577 (64.7587%)\n\t9: \"Ticket_item\" CATEGORICAL has-dict vocab-size:17 num-oods:28 (3.14254%) most-frequent:\"NONE\" 665 (74.6352%)\n\t10: \"Ticket_number\" CATEGORICAL has-dict vocab-size:9 num-oods:671 (75.3086%) most-frequent:\"<OOD>\" 671 (75.3086%)\n\t11: \"__LABEL\" CATEGORICAL integerized vocab-size:3 no-ood-item\n\nNUMERICAL: 5 (41.6667%)\n\t0: \"Age\" NUMERICAL num-nas:177 (19.8653%) mean:29.6991 min:0.42 max:80 sd:14.5163\n\t3: \"Fare\" NUMERICAL mean:32.2042 min:0 max:512.329 sd:49.6655\n\t5: \"Parch\" NUMERICAL mean:0.381594 min:0 max:6 sd:0.805605\n\t6: \"Pclass\" NUMERICAL mean:2.30864 min:1 max:3 sd:0.835602\n\t8: \"SibSp\" NUMERICAL mean:0.523008 min:0 max:8 sd:1.10212\n\nCATEGORICAL_SET: 1 (8.33333%)\n\t4: \"Name\" CATEGORICAL_SET has-dict vocab-size:86 num-oods:1441 (161.728%) most-frequent:\"<OOD>\" 1441 (161.728%)\n\nTerminology:\n\tnas: Number of non-available (i.e. missing) values.\n\tood: Out of dictionary.\n\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n\ttokenized: The attribute value is obtained through tokenization.\n\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n\tvocab-size: Number of unique values.\n\n[INFO 2024-02-11T11:10:07.937132406+00:00 kernel.cc:790] Configure learner\n[INFO 2024-02-11T11:10:07.937822109+00:00 kernel.cc:804] Training config:\nlearner: \"GRADIENT_BOOSTED_TREES\"\nfeatures: \"^Age$\"\nfeatures: \"^Cabin$\"\nfeatures: \"^Embarked$\"\nfeatures: \"^Fare$\"\nfeatures: \"^Name$\"\nfeatures: \"^Parch$\"\nfeatures: \"^Pclass$\"\nfeatures: \"^Sex$\"\nfeatures: \"^SibSp$\"\nfeatures: \"^Ticket_item$\"\nfeatures: \"^Ticket_number$\"\nlabel: \"^__LABEL$\"\ntask: CLASSIFICATION\nrandom_seed: 1234\nmetadata {\n  framework: \"TF Keras\"\n}\npure_serving_model: false\n[yggdrasil_decision_forests.model.gradient_boosted_trees.proto.gradient_boosted_trees_config] {\n  num_trees: 2000\n  decision_tree {\n    max_depth: 6\n    min_examples: 1\n    in_split_min_examples_check: true\n    keep_non_leaf_label_distribution: true\n    num_candidate_attributes: -1\n    missing_value_policy: GLOBAL_IMPUTATION\n    allow_na_conditions: false\n    categorical_set_greedy_forward {\n      sampling: 0.1\n      max_num_items: -1\n      min_item_frequency: 1\n    }\n    growing_strategy_best_first_global {\n    }\n    categorical {\n      random {\n      }\n    }\n    sparse_oblique_split {\n      num_projections_exponent: 2\n      normalization: MIN_MAX\n    }\n    internal {\n      sorting_strategy: PRESORTED\n    }\n    uplift {\n      min_examples_in_treatment: 5\n      split_score: KULLBACK_LEIBLER\n    }\n  }\n  shrinkage: 0.05\n  loss: DEFAULT\n  validation_set_ratio: 0.1\n  validation_interval_in_trees: 1\n  early_stopping: VALIDATION_LOSS_INCREASE\n  early_stopping_num_trees_look_ahead: 30\n  l2_regularization: 0\n  lambda_loss: 1\n  mart {\n  }\n  adapt_subsample_for_maximum_training_duration: false\n  l1_regularization: 0\n  use_hessian_gain: false\n  l2_regularization_categorical: 1\n  stochastic_gradient_boosting {\n    ratio: 1\n  }\n  apply_link_function: true\n  compute_permutation_variable_importance: true\n  binary_focal_loss_options {\n    misprediction_exponent: 2\n    positive_sample_coefficient: 0.5\n  }\n  early_stopping_initial_iteration: 10\n}\n\n[INFO 2024-02-11T11:10:07.938119251+00:00 kernel.cc:807] Deployment config:\ncache_path: \"/tmp/tmp43nud6l9/working_cache\"\nnum_threads: 4\ntry_resume_training: true\n\n[INFO 2024-02-11T11:10:07.938395692+00:00 kernel.cc:868] Train model\n[INFO 2024-02-11T11:10:09.069133626+00:00 early_stopping.cc:53] Early stop of the training because the validation loss does not decrease anymore. Best valid-loss: 1.05021\n[INFO 2024-02-11T11:10:09.0864529+00:00 decision_forest.cc:661] Model loaded with 42 root(s), 2562 node(s), and 10 input feature(s).\n[INFO 2024-02-11T11:10:09.086560409+00:00 abstract_model.cc:1311] Engine \"GradientBoostedTreesGeneric\" built\n[INFO 2024-02-11T11:10:09.089944911+00:00 feature_importance.cc:192] Running 12 features on 4 threads with 1 rounds\n[INFO 2024-02-11T11:10:09.095568757+00:00 [INFO decision_forest.cc:2024-02-11T11:10:09.095645041+00:00661 decision_forest.cc:661] Model loaded with ] Model loaded with 42 root(s), 2562 node(s), and 10 input feature(s).\n42 root(s), 2562 node(s), and 10 input feature(s).\n[INFO 2024-02-11T11:10:09.09607034+00:00 decision_forest.cc:661] Model loaded with 42 root(s), 2562 node(s), and 10 input feature(s).\n[INFO [INFO 2024-02-11T11:10:09.10101738+00:00 decision_forest.cc:2024-02-11T11:10:09.10104621+00:00661 decision_forest.cc:661] Model loaded with 42 root(s), 2562 node(s), and 10 input feature(s).\n[INFO 2024-02-11T11:10:09.101486568+00:00 decision_forest.cc:661] Model loaded with 42 root(s), 2562 node(s), and 10 input feature(s).\n[INFO 2024-02-11T11:10:09.105308452+00:00 decision_forest.cc:661] Model loaded with 42 root(s), 2562 node(s), and 10 input feature(s).\n[INFO 2024-02-11T11:10:09.106378331+00:00 decision_forest.cc:661] Model loaded with 42 root(s), 2562 node(s), and 10 input feature(s).\n[INFO 2024-02-11T11:10:09.106825353+00:00 decision_forest.cc:661] Model loaded with 42 root(s), 2562 node(s), and 10 input feature(s).\n] Model loaded with 42 root(s), 2562 node(s), and 10 input feature(s).\n[INFO 2024-02-11T11:10:09.111469645+00:00 decision_forest.cc:661] Model loaded with 42 root(s), 2562 node(s), and 10 input feature(s).\n[INFO 2024-02-11T11:10:09.112376484+00:00 decision_forest.cc:661] Model loaded with 42 root(s), 2562 node(s), and 10 input feature(s).\n[INFO 2024-02-11T11:10:09.114711221+00:00 kernel.cc:905] Export model in log directory: /tmp/tmp43nud6l9 with prefix 94ea911ebacc48f4\n[INFO 2024-02-11T11:10:09.119194157+00:00 kernel.cc:923] Save model in resources\n[INFO 2024-02-11T11:10:09.124506009+00:00 abstract_model.cc:849] Model self evaluation:\nTask: CLASSIFICATION\nLabel: __LABEL\nLoss (BINOMIAL_LOG_LIKELIHOOD): 1.05021\n\nAccuracy: 0.73913  CI95[W][0 1]\nErrorRate: : 0.26087\n\n\nConfusion Table:\ntruth\\prediction\n   0   1   2\n0  0   0   0\n1  0  42   8\n2  0  16  26\nTotal: 92\n\nOne vs other classes:\n\n[INFO 2024-02-11T11:10:09.157414123+00:00 kernel.cc:1214] Loading model from path /tmp/tmp43nud6l9/model/ with prefix 94ea911ebacc48f4\n[INFO 2024-02-11T11:10:09.174257361+00:00 decision_forest.cc:661] Model loaded with 42 root(s), 2562 node(s), and 10 input feature(s).\n[INFO 2024-02-11T11:10:09.174329679+00:00 abstract_model.cc:1311] Engine \"GradientBoostedTreesGeneric\" built\n[INFO 2024-02-11T11:10:09.174375768+00:00 kernel.cc:1046] Use fast generic engine\n","output_type":"stream"},{"name":"stdout","text":"Model trained in 0:00:01.256330\nCompiling model...\nModel compiled.\nAccuracy: 0.739130437374115 Loss:1.0502052307128906\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's look at the model and you can also notice the information about variable importance that the model figured out","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-02-11T11:10:32.852627Z","iopub.execute_input":"2024-02-11T11:10:32.853128Z","iopub.status.idle":"2024-02-11T11:10:32.876853Z","shell.execute_reply.started":"2024-02-11T11:10:32.853084Z","shell.execute_reply":"2024-02-11T11:10:32.875390Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model: \"gradient_boosted_trees_model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n=================================================================\nTotal params: 1\nTrainable params: 0\nNon-trainable params: 1\n_________________________________________________________________\nType: \"GRADIENT_BOOSTED_TREES\"\nTask: CLASSIFICATION\nLabel: \"__LABEL\"\n\nInput Features (11):\n\tAge\n\tCabin\n\tEmbarked\n\tFare\n\tName\n\tParch\n\tPclass\n\tSex\n\tSibSp\n\tTicket_item\n\tTicket_number\n\nNo weights\n\nVariable Importance: INV_MEAN_MIN_DEPTH:\n    1.           \"Sex\"  0.611555 ################\n    2.           \"Age\"  0.343173 ######\n    3.          \"Fare\"  0.275439 ####\n    4.          \"Name\"  0.189713 #\n    5.        \"Pclass\"  0.172931 \n    6.   \"Ticket_item\"  0.169311 \n    7. \"Ticket_number\"  0.165249 \n    8.         \"Parch\"  0.164456 \n    9.      \"Embarked\"  0.163076 \n   10.         \"SibSp\"  0.158163 \n\nVariable Importance: MEAN_DECREASE_IN_ACCURACY:\n    1.           \"Sex\"  0.141304 ################\n    2.        \"Pclass\"  0.032609 #####\n    3.          \"Name\"  0.010870 ###\n    4. \"Ticket_number\"  0.000000 ##\n    5.           \"Age\" -0.010870 #\n    6.         \"Cabin\" -0.010870 #\n    7.      \"Embarked\" -0.010870 #\n    8.          \"Fare\" -0.010870 #\n    9.         \"Parch\" -0.010870 #\n   10.   \"Ticket_item\" -0.010870 #\n   11.         \"SibSp\" -0.032609 \n\nVariable Importance: MEAN_DECREASE_IN_AP_2_VS_OTHERS:\n    1.           \"Sex\"  0.237107 ################\n    2.        \"Pclass\"  0.073975 #####\n    3.          \"Fare\"  0.031059 ###\n    4.   \"Ticket_item\"  0.010742 #\n    5.           \"Age\"  0.005835 #\n    6.          \"Name\"  0.003700 #\n    7.      \"Embarked\"  0.002511 #\n    8. \"Ticket_number\" -0.000450 #\n    9.         \"Cabin\" -0.001098 #\n   10.         \"Parch\" -0.007722 \n   11.         \"SibSp\" -0.021270 \n\nVariable Importance: MEAN_DECREASE_IN_AUC_2_VS_OTHERS:\n    1.           \"Sex\"  0.129286 ################\n    2.        \"Pclass\"  0.049762 ######\n    3.          \"Fare\"  0.046667 ######\n    4.           \"Age\"  0.020476 ###\n    5.   \"Ticket_item\"  0.013810 ##\n    6.      \"Embarked\"  0.011429 ##\n    7.         \"SibSp\" -0.000238 \n    8.         \"Cabin\" -0.000476 \n    9. \"Ticket_number\" -0.001905 \n   10.         \"Parch\" -0.003810 \n   11.          \"Name\" -0.007381 \n\nVariable Importance: MEAN_DECREASE_IN_PRAUC_2_VS_OTHERS:\n    1.           \"Sex\"  0.230084 ################\n    2.        \"Pclass\"  0.071692 #####\n    3.          \"Fare\"  0.030423 ###\n    4.   \"Ticket_item\"  0.010767 ##\n    5.           \"Age\"  0.005700 #\n    6.          \"Name\"  0.003525 #\n    7.      \"Embarked\"  0.002365 #\n    8. \"Ticket_number\" -0.000452 #\n    9.         \"Cabin\" -0.001083 #\n   10.         \"Parch\" -0.007577 \n   11.         \"SibSp\" -0.021451 \n\nVariable Importance: NUM_AS_ROOT:\n    1.  \"Sex\" 37.000000 ################\n    2. \"Name\"  5.000000 \n\nVariable Importance: NUM_NODES:\n    1.           \"Age\" 587.000000 ################\n    2.          \"Fare\" 381.000000 ##########\n    3.          \"Name\" 92.000000 ##\n    4.   \"Ticket_item\" 62.000000 #\n    5.           \"Sex\" 40.000000 #\n    6.         \"Parch\" 36.000000 \n    7. \"Ticket_number\" 23.000000 \n    8.      \"Embarked\" 22.000000 \n    9.        \"Pclass\" 15.000000 \n   10.         \"SibSp\"  2.000000 \n\nVariable Importance: SUM_SCORE:\n    1.           \"Sex\" 522.801941 ################\n    2.           \"Age\" 447.179479 #############\n    3.          \"Fare\" 355.389805 ##########\n    4.          \"Name\" 69.770576 ##\n    5.        \"Pclass\" 35.931115 #\n    6.   \"Ticket_item\" 29.751246 \n    7.         \"Parch\" 22.243409 \n    8. \"Ticket_number\" 19.027313 \n    9.      \"Embarked\" 11.896268 \n   10.         \"SibSp\"  0.098062 \n\n\n\nLoss: BINOMIAL_LOG_LIKELIHOOD\nValidation loss value: 1.05021\nNumber of trees per iteration: 1\nNode format: NOT_SET\nNumber of trees: 42\nTotal number of nodes: 2562\n\nNumber of nodes by tree:\nCount: 42 Average: 61 StdDev: 0\nMin: 61 Max: 61 Ignored: 0\n----------------------------------------------\n[ 61, 61] 42 100.00% 100.00% ##########\n\nDepth by leafs:\nCount: 1302 Average: 5.32565 StdDev: 0.925788\nMin: 2 Max: 6 Ignored: 0\n----------------------------------------------\n[ 2, 3)   3   0.23%   0.23%\n[ 3, 4)  78   5.99%   6.22% #\n[ 4, 5) 163  12.52%  18.74% ##\n[ 5, 6) 306  23.50%  42.24% ####\n[ 6, 6] 752  57.76% 100.00% ##########\n\nNumber of training obs by leaf:\nCount: 1302 Average: 0 StdDev: 0\nMin: 0 Max: 0 Ignored: 0\n----------------------------------------------\n[ 0, 0] 1302 100.00% 100.00% ##########\n\nAttribute in nodes:\n\t587 : Age [NUMERICAL]\n\t381 : Fare [NUMERICAL]\n\t92 : Name [CATEGORICAL_SET]\n\t62 : Ticket_item [CATEGORICAL]\n\t40 : Sex [CATEGORICAL]\n\t36 : Parch [NUMERICAL]\n\t23 : Ticket_number [CATEGORICAL]\n\t22 : Embarked [CATEGORICAL]\n\t15 : Pclass [NUMERICAL]\n\t2 : SibSp [NUMERICAL]\n\nAttribute in nodes with depth <= 0:\n\t37 : Sex [CATEGORICAL]\n\t5 : Name [CATEGORICAL_SET]\n\nAttribute in nodes with depth <= 1:\n\t41 : Age [NUMERICAL]\n\t37 : Sex [CATEGORICAL]\n\t29 : Fare [NUMERICAL]\n\t11 : Pclass [NUMERICAL]\n\t5 : Name [CATEGORICAL_SET]\n\t2 : Ticket_number [CATEGORICAL]\n\t1 : Parch [NUMERICAL]\n\nAttribute in nodes with depth <= 2:\n\t121 : Age [NUMERICAL]\n\t80 : Fare [NUMERICAL]\n\t37 : Sex [CATEGORICAL]\n\t19 : Name [CATEGORICAL_SET]\n\t11 : Pclass [NUMERICAL]\n\t8 : Ticket_item [CATEGORICAL]\n\t7 : Parch [NUMERICAL]\n\t6 : Ticket_number [CATEGORICAL]\n\t2 : Embarked [CATEGORICAL]\n\nAttribute in nodes with depth <= 3:\n\t239 : Age [NUMERICAL]\n\t166 : Fare [NUMERICAL]\n\t37 : Sex [CATEGORICAL]\n\t33 : Name [CATEGORICAL_SET]\n\t19 : Ticket_item [CATEGORICAL]\n\t17 : Parch [NUMERICAL]\n\t14 : Ticket_number [CATEGORICAL]\n\t11 : Pclass [NUMERICAL]\n\t7 : Embarked [CATEGORICAL]\n\nAttribute in nodes with depth <= 5:\n\t587 : Age [NUMERICAL]\n\t381 : Fare [NUMERICAL]\n\t92 : Name [CATEGORICAL_SET]\n\t62 : Ticket_item [CATEGORICAL]\n\t40 : Sex [CATEGORICAL]\n\t36 : Parch [NUMERICAL]\n\t23 : Ticket_number [CATEGORICAL]\n\t22 : Embarked [CATEGORICAL]\n\t15 : Pclass [NUMERICAL]\n\t2 : SibSp [NUMERICAL]\n\nCondition type in nodes:\n\t1021 : ObliqueCondition\n\t165 : ContainsBitmapCondition\n\t74 : ContainsCondition\nCondition type in nodes with depth <= 0:\n\t41 : ContainsBitmapCondition\n\t1 : ContainsCondition\nCondition type in nodes with depth <= 1:\n\t82 : ObliqueCondition\n\t43 : ContainsBitmapCondition\n\t1 : ContainsCondition\nCondition type in nodes with depth <= 2:\n\t219 : ObliqueCondition\n\t60 : ContainsBitmapCondition\n\t12 : ContainsCondition\nCondition type in nodes with depth <= 3:\n\t433 : ObliqueCondition\n\t85 : ContainsBitmapCondition\n\t25 : ContainsCondition\nCondition type in nodes with depth <= 5:\n\t1021 : ObliqueCondition\n\t165 : ContainsBitmapCondition\n\t74 : ContainsCondition\n\nTraining logs:\nNumber of iteration to final model: 42\n\tIter:1 train-loss:1.261200 valid-loss:1.366915  train-accuracy:0.624531 valid-accuracy:0.543478\n\tIter:2 train-loss:1.203236 valid-loss:1.331314  train-accuracy:0.624531 valid-accuracy:0.543478\n\tIter:3 train-loss:1.152296 valid-loss:1.300096  train-accuracy:0.624531 valid-accuracy:0.543478\n\tIter:4 train-loss:1.103903 valid-loss:1.275920  train-accuracy:0.624531 valid-accuracy:0.543478\n\tIter:5 train-loss:1.061592 valid-loss:1.254156  train-accuracy:0.816020 valid-accuracy:0.728261\n\tIter:6 train-loss:1.021464 valid-loss:1.228286  train-accuracy:0.843554 valid-accuracy:0.717391\n\tIter:16 train-loss:0.749953 valid-loss:1.101772  train-accuracy:0.921151 valid-accuracy:0.728261\n\tIter:26 train-loss:0.593136 valid-loss:1.061535  train-accuracy:0.934919 valid-accuracy:0.750000\n\tIter:36 train-loss:0.490701 valid-loss:1.057855  train-accuracy:0.941176 valid-accuracy:0.728261\n\tIter:46 train-loss:0.416346 valid-loss:1.054336  train-accuracy:0.948686 valid-accuracy:0.750000\n\tIter:56 train-loss:0.361065 valid-loss:1.066538  train-accuracy:0.954944 valid-accuracy:0.750000\n\tIter:66 train-loss:0.321980 valid-loss:1.082563  train-accuracy:0.959950 valid-accuracy:0.739130\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Make predictions","metadata":{}},{"cell_type":"code","source":"def prediction_to_kaggle_format(model, threshold=0.5):\n    proba_survive = model.predict(serving_ds, verbose=0)[:,0]\n    return pd.DataFrame({\n        \"PassengerId\": serving_df[\"PassengerId\"],\n        \"Survived\": (proba_survive >= threshold).astype(int)\n    })\n\ndef make_submission(kaggle_predictions):\n    path=\"/kaggle/working/submission.csv\"\n    kaggle_predictions.to_csv(path, index=False)\n    print(f\"Submission exported to {path}\")\n    \nkaggle_predictions = prediction_to_kaggle_format(model)\nmake_submission(kaggle_predictions)\n!head /kaggle/working/submission.csv","metadata":{"execution":{"iopub.status.busy":"2024-02-11T11:11:07.969074Z","iopub.execute_input":"2024-02-11T11:11:07.969617Z","iopub.status.idle":"2024-02-11T11:11:09.303618Z","shell.execute_reply.started":"2024-02-11T11:11:07.969574Z","shell.execute_reply":"2024-02-11T11:11:09.302186Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Submission exported to /kaggle/working/submission.csv\nPassengerId,Survived\n892,0\n893,0\n894,0\n895,0\n896,0\n897,0\n898,0\n899,0\n900,1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training a model with hyperparameter tunning\n\nHyper-parameter tuning is enabled by specifying the tuner constructor argument of the model. The tuner object contains all the configuration of the tuner (search space, optimizer, trial and objective).\n","metadata":{}},{"cell_type":"code","source":"tuner = tfdf.tuner.RandomSearch(num_trials=1000)\ntuner.choice(\"min_examples\", [2, 5, 7, 10])\ntuner.choice(\"categorical_algorithm\", [\"CART\", \"RANDOM\"])\n\nlocal_search_space = tuner.choice(\"growing_strategy\", [\"LOCAL\"])\nlocal_search_space.choice(\"max_depth\", [3, 4, 5, 6, 8])\n\nglobal_search_space = tuner.choice(\"growing_strategy\", [\"BEST_FIRST_GLOBAL\"], merge=True)\nglobal_search_space.choice(\"max_num_nodes\", [16, 32, 64, 128, 256])\n\n#tuner.choice(\"use_hessian_gain\", [True, False])\ntuner.choice(\"shrinkage\", [0.02, 0.05, 0.10, 0.15])\ntuner.choice(\"num_candidate_attributes_ratio\", [0.2, 0.5, 0.9, 1.0])\n\n\ntuner.choice(\"split_axis\", [\"AXIS_ALIGNED\"])\noblique_space = tuner.choice(\"split_axis\", [\"SPARSE_OBLIQUE\"], merge=True)\noblique_space.choice(\"sparse_oblique_normalization\",\n                     [\"NONE\", \"STANDARD_DEVIATION\", \"MIN_MAX\"])\noblique_space.choice(\"sparse_oblique_weights\", [\"BINARY\", \"CONTINUOUS\"])\noblique_space.choice(\"sparse_oblique_num_projections_exponent\", [1.0, 1.5])\n\n# Tune the model. Notice the `tuner=tuner`.\ntuned_model = tfdf.keras.GradientBoostedTreesModel(tuner=tuner)\ntuned_model.fit(train_ds, verbose=0)\n\ntuned_self_evaluation = tuned_model.make_inspector().evaluation()\nprint(f\"Accuracy: {tuned_self_evaluation.accuracy} Loss:{tuned_self_evaluation.loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:23:13.249675Z","iopub.execute_input":"2023-04-17T15:23:13.251376Z","iopub.status.idle":"2023-04-17T15:25:19.611729Z","shell.execute_reply.started":"2023-04-17T15:23:13.251306Z","shell.execute_reply":"2023-04-17T15:25:19.610154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the last line in the cell above, you can see the accuracy is higher than previously with default parameters and parameters set by hand.\n\nThis is the main idea behing hyperparameter tuning.\n\nFor more information you can follow this tutorial: [Automated hyper-parameter tuning](https://www.tensorflow.org/decision_forests/tutorials/automatic_tuning_colab)","metadata":{}},{"cell_type":"markdown","source":"# Making an ensemble\n\nHere you'll create 100 models with different seeds and combine their results\n\nThis approach removes a little bit the random aspects related to creating ML models\n\nIn the GBT creation is used the `honest` parameter. It will use different training examples to infer the structure and the leaf values. This regularization technique trades examples for bias estimates.","metadata":{}},{"cell_type":"code","source":"predictions = None\nnum_predictions = 0\n\nfor i in range(100):\n    print(f\"i:{i}\")\n    # Possible models: GradientBoostedTreesModel or RandomForestModel\n    model = tfdf.keras.GradientBoostedTreesModel(\n        verbose=0, # Very few logs\n        features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n        exclude_non_specified_features=True, # Only use the features in \"features\"\n\n        #min_examples=1,\n        #categorical_algorithm=\"RANDOM\",\n        ##max_depth=4,\n        #shrinkage=0.05,\n        ##num_candidate_attributes_ratio=0.2,\n        #split_axis=\"SPARSE_OBLIQUE\",\n        #sparse_oblique_normalization=\"MIN_MAX\",\n        #sparse_oblique_num_projections_exponent=2.0,\n        #num_trees=2000,\n        ##validation_ratio=0.0,\n        random_seed=i,\n        honest=True,\n    )\n    model.fit(train_ds)\n    \n    sub_predictions = model.predict(serving_ds, verbose=0)[:,0]\n    if predictions is None:\n        predictions = sub_predictions\n    else:\n        predictions += sub_predictions\n    num_predictions += 1\n\npredictions/=num_predictions\n\nkaggle_predictions = pd.DataFrame({\n        \"PassengerId\": serving_df[\"PassengerId\"],\n        \"Survived\": (predictions >= 0.5).astype(int)\n    })\n\nmake_submission(kaggle_predictions)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T15:28:37.557745Z","iopub.execute_input":"2023-04-17T15:28:37.558172Z","iopub.status.idle":"2023-04-17T15:28:52.809698Z","shell.execute_reply.started":"2023-04-17T15:28:37.55813Z","shell.execute_reply":"2023-04-17T15:28:52.808193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What is next\n\nIf you want to learn more about TensorFlow Decision Forests and its advanced features, you can follow the official documentation [here](https://www.tensorflow.org/decision_forests) ","metadata":{}}]}